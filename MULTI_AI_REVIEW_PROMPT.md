# Multi-AI Review: Public Domain Monetization Plan

## INSTRUCTIONS FOR ALL REVIEWERS

You are part of a multi-AI review panel evaluating a public domain monetization plan. Each reviewer has a specific role. Be **brutally honest**, **contrarian**, and **constructively critical**. The goal is to find flaws BEFORE execution, not to validate the plan.

---

## THE PLAN SUMMARY

**Objective:** Monetize 1930 works entering US public domain on January 1, 2026 through KDP annotated editions.

**First 3 Titles:**
1. The Maltese Falcon (Dashiell Hammett)
2. Strong Poison (Dorothy L. Sayers)
3. Last and First Men (Olaf Stapledon)

**Approach:**
- Create "Reader's Companion Editions" with original content (introductions, summaries, character guides, discussion questions)
- Sell on Amazon KDP at $4.99 (ebook) / $14.99 (paperback)
- 35% royalty on ebooks (PD content rule)
- Target 20+ titles in first year

**Revenue Projection:**
- $85/month per title (conservative)
- $1,000/month with 12-15 active titles
- First-year total: $5,000-7,000

---

## REVIEWER ROLES & QUESTIONS

### 1. CONTRARIAN REVIEWER
**Role:** Find everything wrong with this plan

**Questions to answer:**
1. What assumptions are being made without evidence?
2. What could go wrong that isn't being discussed?
3. Where is the plan overconfident?
4. What market realities are being ignored?
5. What legal risks are understated?
6. What platform risks (KDP) aren't fully considered?
7. What competitor dynamics are missed?
8. Is the timeline realistic for a solo creator?
9. Are the revenue projections backed by actual data?
10. What would cause this entire approach to fail?

**Your mandate:** Be harsh. Be skeptical. Find the holes.

---

### 2. STRATEGIC REVIEWER
**Role:** Evaluate the big picture

**Questions to answer:**
1. Does this plan align with the user's actual goals?
2. Is this the best use of their time vs. alternatives?
3. What's the total addressable market for annotated PD editions?
4. What's the competitive moat (or lack thereof)?
5. Is this a business or just a project?
6. What's the 3-year trajectory?
7. How does this scale (or not)?
8. What opportunities are being missed?
9. What's the exit strategy?
10. Is the effort-to-reward ratio favorable vs. alternatives?

**Your mandate:** Think like an investor. Be direct about whether this is worth doing.

---

### 3. LEGAL REVIEWER
**Role:** Verify all legal claims

**Questions to answer:**
1. Are the PD dates correct (1930 works → PD January 1, 2026)?
2. Are the EU copyright term calculations accurate (life + 70)?
3. Is the trademark analysis sound (Maltese Falcon, Nancy Drew, etc.)?
4. Are the legal precedents cited correctly (Warner Bros. v. CBS, Dastar, Klinger)?
5. Is the nominative fair use analysis correct for metadata?
6. Are the KDP compliance requirements accurately stated?
7. Are there hidden legal risks not discussed?
8. Is the dispute response approach actually defensible?
9. What international complications are being underestimated?
10. Would a real IP attorney sign off on this approach?

**Your mandate:** Be precise. Cite sources. Flag every uncertainty.

---

### 4. MARKET REVIEWER
**Role:** Verify market assumptions with data

**Questions to answer:**
1. What's the actual market size for annotated classic editions on Amazon?
2. How many competing products exist for each target title?
3. What are typical sales volumes for similar products (BSR analysis)?
4. Are the pricing assumptions ($4.99/$14.99) competitive?
5. Do the revenue projections ($85/month/title) match market data?
6. What's the realistic customer acquisition cost?
7. How saturated is the "annotated classics" market?
8. What's the search volume for relevant keywords?
9. Are there better/larger markets to target with PD content?
10. What market trends could help or hurt this business?

**Your mandate:** Use data where possible. Be skeptical of optimistic projections.

---

### 5. EXECUTION REVIEWER
**Role:** Assess practical feasibility

**Questions to answer:**
1. Are the time estimates (18-22 hours for Title #1) realistic?
2. What skills does this require that weren't explicitly stated?
3. What's likely to take much longer than expected?
4. What dependencies could block progress?
5. Is the automation actually saving significant time?
6. What manual work is hidden in the "automated" process?
7. What could derail the January 1 launch timeline?
8. Are the tool recommendations (Canva, Atticus, etc.) appropriate?
9. Is the workflow actually optimized or just documented?
10. What would a solo creator with limited time struggle with?

**Your mandate:** Be practical. Focus on real-world execution challenges.

---

### 6. SYNTHESIS REVIEWER
**Role:** Combine all perspectives into actionable recommendations

**Questions to answer:**
1. What are the TOP 5 risks that must be addressed before execution?
2. What are the TOP 3 opportunities being missed?
3. What needs to change before this plan is truly execution-ready?
4. What's the honest probability of achieving the stated goals?
5. What's the minimum viable version of this plan that still makes sense?

**Your mandate:** Prioritize. Be actionable. Don't sugarcoat.

---

## FILES TO REVIEW

The complete plan is in these files:

1. **`2026_public_domain_playbook_v4.md`** - Strategic playbook
2. **`docs/AGENT_SWARM_ARCHITECTURE.md`** - Agent system design
3. **`kits/COMPLETE_PRODUCTION_KIT.md`** - Execution guide
4. **`files (3)/MASTER_TIER1_LIST.md`** - Work selection
5. **`files (3)/PREFILLED_PROMPTS.md`** - Content prompts
6. **`files (3)/KDP_LISTINGS_READY.md`** - Listing templates
7. **`01_PD_DOSSIER.md`** - Legal clearance example
8. **`03_DIFFERENTIATION_PROOF.md`** - Dispute template

---

## OUTPUT FORMAT

Each reviewer should produce a report with:

```markdown
# [ROLE] Review: Public Domain Monetization Plan

## Summary
[2-3 sentence overall assessment]

## Key Findings
1. [Finding 1]
2. [Finding 2]
3. [Finding 3]
...

## Critical Issues (Must Address)
- [Issue 1]: [Why it matters] → [Recommended fix]
- [Issue 2]: [Why it matters] → [Recommended fix]

## Moderate Issues (Should Address)
- [Issue 1]
- [Issue 2]

## Minor Issues (Nice to Fix)
- [Issue 1]
- [Issue 2]

## What's Actually Good
- [Positive 1]
- [Positive 2]

## Bottom Line
[Final verdict: Proceed / Proceed with changes / Reconsider approach]
```

---

## HOW TO USE THIS DOCUMENT

### For Claude/ChatGPT:
1. Share all plan files
2. Assign one reviewer role per session
3. Request structured output
4. Compile all 6 reviews
5. Have a 7th session synthesize across all reviews

### For Multiple AI Services:
- Claude: Contrarian + Legal reviews (strong reasoning)
- ChatGPT: Market + Execution reviews (broad knowledge)
- Gemini: Strategic review (different perspective)
- Perplexity: Market data verification

### For GitHub Actions:
The `multi-ai-review.yml` workflow runs all review prompts automatically on push/PR.

---

## EXPECTED OUTCOMES

After multi-AI review, you should have:

1. **Risk Register:** All identified risks with severity ratings
2. **Issue List:** Prioritized problems to fix
3. **Opportunity List:** Missed opportunities to capture
4. **Revised Plan:** Updated documents addressing critical issues
5. **Go/No-Go Decision:** Clear recommendation on whether to proceed

---

*This multi-AI review process ensures the plan is stress-tested from every angle before execution.*
